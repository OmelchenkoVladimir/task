{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#LSTM --> logit\n",
    "#стоит также попробовать усреднять состояния LSTM в течение всего обучения\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import string\n",
    "from ast import literal_eval\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=195782, size=300, alpha=0.025)\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dct = FastText.load(r'araneum/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "size = 300 #dct.size\n",
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def seq(tokens):\n",
    "    sentence = literal_eval(tokens)\n",
    "    res = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            res.append(dct.wv[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(res) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train['seq'] = train['tokens'].apply(seq)\n",
    "test['seq'] = test['tokens'].apply(seq)\n",
    "#в обоих случаях None нет, поэтому сохраняем все предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel dct\n",
    "#запускалось на 8GB RAM, поэтому это необходимо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(train['seq'].tolist(), maxlen = 25, dtype = 'float32') #при максимальной длине получаем MemoryError\n",
    "X_test = sequence.pad_sequences(test['seq'].tolist(), maxlen = 25, dtype = 'float32')\n",
    "#особой разницы между pre и pos padding замечено не было"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train.loc[:,'author'].map({'Гоголь' : 0, 'Гегель' : 1})\n",
    "y_test = test.loc[:,'author'].map({'Гоголь' : 0, 'Гегель' : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape = (25, 300)))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25702/25702 [==============================] - 20s 785us/step - loss: 0.1900 - acc: 0.9287\n",
      "Epoch 2/2\n",
      "25702/25702 [==============================] - 21s 828us/step - loss: 0.1256 - acc: 0.9550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2625de10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 64, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16249/16249 [==============================] - 7s 417us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12805612085666696, 0.9521201304695673]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложения, которые можно приписать и Гоголю, и Гегелю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Хотя почтмейстер был очень речист, но и тот, в...</td>\n",
       "      <td>['хотя', 'почтмейстер', 'был', 'очень', 'речис...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Хозяйством нельзя сказать чтобы он занимался, ...</td>\n",
       "      <td>['хозяйством', 'нельзя', 'сказать', 'чтобы', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>«Да, именно недурно», — повторял он.</td>\n",
       "      <td>['да', 'именно', 'недурно', 'повторял', 'он']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Великий упрек был бы историку предлагаемых соб...</td>\n",
       "      <td>['великий', 'упрек', 'был', 'бы', 'историку', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>А имя и отчество?</td>\n",
       "      <td>['а', 'имя', 'и', 'отчество']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>Вы возьмите всякую негодную, последнюю вещь, н...</td>\n",
       "      <td>['вы', 'возьмите', 'всякую', 'негодную', 'посл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>И потому теперь он совершенно обиделся.</td>\n",
       "      <td>['и', 'потому', 'теперь', 'он', 'совершенно', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>— Да шашку-то, — сказал Чичиков и в то же врем...</td>\n",
       "      <td>['да', 'сказал', 'чичиков', 'и', 'в', 'то', 'ж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>— Я приехал вам объявить сообщенное мне извеще...</td>\n",
       "      <td>['я', 'приехал', 'вам', 'объявить', 'сообщенно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>Ведь предмет просто фу-фу.</td>\n",
       "      <td>['ведь', 'предмет', 'просто']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>— Ну, видите ли, я вдруг постигнул ваш характер.</td>\n",
       "      <td>['ну', 'видите', 'ли', 'я', 'вдруг', 'постигну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>— «Никогда, никогда, — говорил управляющий каз...</td>\n",
       "      <td>['никогда', 'никогда', 'говорил', 'управляющий...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>Даже из-за него уже начинали несколько ссорить...</td>\n",
       "      <td>['даже', 'него', 'уже', 'начинали', 'несколько...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>Ну и в книге, и там была бы она также бестолко...</td>\n",
       "      <td>['ну', 'и', 'в', 'книге', 'и', 'там', 'была', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>Он рассердился, приготовился даже задать что-т...</td>\n",
       "      <td>['он', 'рассердился', 'приготовился', 'даже', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>Теперь же решился он во что бы то ни стало доб...</td>\n",
       "      <td>['теперь', 'же', 'решился', 'он', 'во', 'что',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>А переселение можно сделать законным образом, ...</td>\n",
       "      <td>['а', 'переселение', 'можно', 'сделать', 'зако...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158</th>\n",
       "      <td>И вот таким образом составился в голове нашего...</td>\n",
       "      <td>['и', 'вот', 'таким', 'образом', 'составился',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>— Поприще службы моей, — сказал Чичиков, садяс...</td>\n",
       "      <td>['поприще', 'службы', 'моей', 'сказал', 'чичик...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>Платон усмехнулся.</td>\n",
       "      <td>['платон', 'усмехнулся']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>— А имя и отчество?</td>\n",
       "      <td>['а', 'имя', 'и', 'отчество']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>— Да уж в работниках не будете иметь недостатку.</td>\n",
       "      <td>['да', 'уж', 'в', 'работниках', 'не', 'будете'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5597</th>\n",
       "      <td>Но заметна, однако же, была примесь чего-то же...</td>\n",
       "      <td>['но', 'заметна', 'однако', 'же', 'была', 'при...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>Точно как будто бы у них государство какое!</td>\n",
       "      <td>['точно', 'как', 'будто', 'бы', 'у', 'них', 'г...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>Какое разнообразие работ!</td>\n",
       "      <td>['какое', 'разнообразие', 'работ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>— Нет, Павел Иванович, — сказал он, — уж если ...</td>\n",
       "      <td>['нет', 'павел', 'иванович', 'сказал', 'он', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6032</th>\n",
       "      <td>Чичиков понял и то, что с этаким нечего толков...</td>\n",
       "      <td>['чичиков', 'понял', 'и', 'то', 'что', 'с', 'э...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6115</th>\n",
       "      <td>Одряхлел прежде старости своей, и поясница бол...</td>\n",
       "      <td>['одряхлел', 'прежде', 'старости', 'своей', 'и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6377</th>\n",
       "      <td>— Жалок он мне, право, жалок!</td>\n",
       "      <td>['жалок', 'он', 'мне', 'право', 'жалок']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6470</th>\n",
       "      <td>«Говорит этот человек несколько витиевато, но ...</td>\n",
       "      <td>['говорит', 'этот', 'человек', 'несколько', 'в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>Совершено оно будет между солидными людьми вта...</td>\n",
       "      <td>['совершено', 'оно', 'будет', 'между', 'солидн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6610</th>\n",
       "      <td>«Понимаю», — подумал Чичиков и сказал: — В сам...</td>\n",
       "      <td>['понимаю', 'подумал', 'чичиков', 'и', 'сказал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>Строится в одном месте церковь доброхотным дат...</td>\n",
       "      <td>['строится', 'в', 'одном', 'месте', 'церковь',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7755</th>\n",
       "      <td>Но здесь должно брать ничто в его неопределенн...</td>\n",
       "      <td>['но', 'здесь', 'должно', 'брать', 'ничто', 'в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>сам представляет собой одну из дурных манер ре...</td>\n",
       "      <td>['сам', 'представляет', 'собой', 'одну', 'из',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8777</th>\n",
       "      <td>Одно и пустота составляют для-себя-бытие в его...</td>\n",
       "      <td>['одно', 'и', 'пустота', 'составляют', 'в', 'е...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>Однако в арифметике не выделяется нить руковод...</td>\n",
       "      <td>['однако', 'в', 'арифметике', 'не', 'выделяетс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11294</th>\n",
       "      <td>Или отрицательно А не может быть в одно и то ж...</td>\n",
       "      <td>['или', 'отрицательно', 'а', 'не', 'может', 'б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16065</th>\n",
       "      <td>- При этом следует начать во-первых с рассмотр...</td>\n",
       "      <td>['при', 'этом', 'следует', 'начать', 'с', 'рас...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "88     Хотя почтмейстер был очень речист, но и тот, в...   \n",
       "197    Хозяйством нельзя сказать чтобы он занимался, ...   \n",
       "199                 «Да, именно недурно», — повторял он.   \n",
       "429    Великий упрек был бы историку предлагаемых соб...   \n",
       "713                                    А имя и отчество?   \n",
       "792    Вы возьмите всякую негодную, последнюю вещь, н...   \n",
       "1427             И потому теперь он совершенно обиделся.   \n",
       "1610   — Да шашку-то, — сказал Чичиков и в то же врем...   \n",
       "1684   — Я приехал вам объявить сообщенное мне извеще...   \n",
       "1977                          Ведь предмет просто фу-фу.   \n",
       "2434    — Ну, видите ли, я вдруг постигнул ваш характер.   \n",
       "2888   — «Никогда, никогда, — говорил управляющий каз...   \n",
       "3031   Даже из-за него уже начинали несколько ссорить...   \n",
       "3143   Ну и в книге, и там была бы она также бестолко...   \n",
       "3770   Он рассердился, приготовился даже задать что-т...   \n",
       "4057   Теперь же решился он во что бы то ни стало доб...   \n",
       "4154   А переселение можно сделать законным образом, ...   \n",
       "4158   И вот таким образом составился в голове нашего...   \n",
       "4853   — Поприще службы моей, — сказал Чичиков, садяс...   \n",
       "5463                                  Платон усмехнулся.   \n",
       "5485                                 — А имя и отчество?   \n",
       "5557    — Да уж в работниках не будете иметь недостатку.   \n",
       "5597   Но заметна, однако же, была примесь чего-то же...   \n",
       "5771         Точно как будто бы у них государство какое!   \n",
       "5920                           Какое разнообразие работ!   \n",
       "5954   — Нет, Павел Иванович, — сказал он, — уж если ...   \n",
       "6032   Чичиков понял и то, что с этаким нечего толков...   \n",
       "6115   Одряхлел прежде старости своей, и поясница бол...   \n",
       "6377                       — Жалок он мне, право, жалок!   \n",
       "6470   «Говорит этот человек несколько витиевато, но ...   \n",
       "6523   Совершено оно будет между солидными людьми вта...   \n",
       "6610   «Понимаю», — подумал Чичиков и сказал: — В сам...   \n",
       "6834   Строится в одном месте церковь доброхотным дат...   \n",
       "7755   Но здесь должно брать ничто в его неопределенн...   \n",
       "7926   сам представляет собой одну из дурных манер ре...   \n",
       "8777   Одно и пустота составляют для-себя-бытие в его...   \n",
       "9253   Однако в арифметике не выделяется нить руковод...   \n",
       "11294  Или отрицательно А не может быть в одно и то ж...   \n",
       "16065  - При этом следует начать во-первых с рассмотр...   \n",
       "\n",
       "                                                  tokens  \n",
       "88     ['хотя', 'почтмейстер', 'был', 'очень', 'речис...  \n",
       "197    ['хозяйством', 'нельзя', 'сказать', 'чтобы', '...  \n",
       "199        ['да', 'именно', 'недурно', 'повторял', 'он']  \n",
       "429    ['великий', 'упрек', 'был', 'бы', 'историку', ...  \n",
       "713                        ['а', 'имя', 'и', 'отчество']  \n",
       "792    ['вы', 'возьмите', 'всякую', 'негодную', 'посл...  \n",
       "1427   ['и', 'потому', 'теперь', 'он', 'совершенно', ...  \n",
       "1610   ['да', 'сказал', 'чичиков', 'и', 'в', 'то', 'ж...  \n",
       "1684   ['я', 'приехал', 'вам', 'объявить', 'сообщенно...  \n",
       "1977                       ['ведь', 'предмет', 'просто']  \n",
       "2434   ['ну', 'видите', 'ли', 'я', 'вдруг', 'постигну...  \n",
       "2888   ['никогда', 'никогда', 'говорил', 'управляющий...  \n",
       "3031   ['даже', 'него', 'уже', 'начинали', 'несколько...  \n",
       "3143   ['ну', 'и', 'в', 'книге', 'и', 'там', 'была', ...  \n",
       "3770   ['он', 'рассердился', 'приготовился', 'даже', ...  \n",
       "4057   ['теперь', 'же', 'решился', 'он', 'во', 'что',...  \n",
       "4154   ['а', 'переселение', 'можно', 'сделать', 'зако...  \n",
       "4158   ['и', 'вот', 'таким', 'образом', 'составился',...  \n",
       "4853   ['поприще', 'службы', 'моей', 'сказал', 'чичик...  \n",
       "5463                            ['платон', 'усмехнулся']  \n",
       "5485                       ['а', 'имя', 'и', 'отчество']  \n",
       "5557   ['да', 'уж', 'в', 'работниках', 'не', 'будете'...  \n",
       "5597   ['но', 'заметна', 'однако', 'же', 'была', 'при...  \n",
       "5771   ['точно', 'как', 'будто', 'бы', 'у', 'них', 'г...  \n",
       "5920                  ['какое', 'разнообразие', 'работ']  \n",
       "5954   ['нет', 'павел', 'иванович', 'сказал', 'он', '...  \n",
       "6032   ['чичиков', 'понял', 'и', 'то', 'что', 'с', 'э...  \n",
       "6115   ['одряхлел', 'прежде', 'старости', 'своей', 'и...  \n",
       "6377            ['жалок', 'он', 'мне', 'право', 'жалок']  \n",
       "6470   ['говорит', 'этот', 'человек', 'несколько', 'в...  \n",
       "6523   ['совершено', 'оно', 'будет', 'между', 'солидн...  \n",
       "6610   ['понимаю', 'подумал', 'чичиков', 'и', 'сказал...  \n",
       "6834   ['строится', 'в', 'одном', 'месте', 'церковь',...  \n",
       "7755   ['но', 'здесь', 'должно', 'брать', 'ничто', 'в...  \n",
       "7926   ['сам', 'представляет', 'собой', 'одну', 'из',...  \n",
       "8777   ['одно', 'и', 'пустота', 'составляют', 'в', 'е...  \n",
       "9253   ['однако', 'в', 'арифметике', 'не', 'выделяетс...  \n",
       "11294  ['или', 'отрицательно', 'а', 'не', 'может', 'б...  \n",
       "16065  ['при', 'этом', 'следует', 'начать', 'с', 'рас...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['res'] = model.predict(X_test)\n",
    "\n",
    "test[abs(test['res'] - 0.5) < 0.01][['sentence', 'tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 0.1\n",
    "tmp = test[abs(test['res'] - 0.5) < x][['sentence', 'tokens']]  #Предложения, которые можно приписать и Гоголю, и Гегелю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel model\n",
    "%xdel train\n",
    "%xdel test\n",
    "%xdel X_train\n",
    "%xdel X_test\n",
    "%xdel y_train\n",
    "%xdel y_test\n",
    "# 8gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "st = []\n",
    "CONTEXT = 2 #в силу очень малого количества данных\n",
    "for s in tmp['tokens']:\n",
    "    sentence = literal_eval(s)\n",
    "    for i in range(len(sentence)):\n",
    "        st.append(sentence[i])\n",
    "        if (i - CONTEXT) >= 0:\n",
    "            data.append({'context':sentence[i-CONTEXT:i], 'target':[sentence[i]]})\n",
    "\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(st)\n",
    "\n",
    "voc_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "data['context_encoded'] = data['context'].apply(tokenizer.texts_to_sequences)\n",
    "data['target_encoded'] = data['target'].apply(tokenizer.texts_to_sequences)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "X = data['context_encoded'].apply(lambda row: np.array([row[0][0], row[1][0]]))\n",
    "y = to_categorical(data['target_encoded'].apply(lambda row: row[0][0]), num_classes = voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, 10, input_length = 2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(voc_size, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 10)             24260     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2426)              123726    \n",
      "=================================================================\n",
      "Total params: 160,186\n",
      "Trainable params: 160,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4983/4983 [==============================] - 4s 704us/step - loss: 7.6322 - acc: 0.0407\n",
      "Epoch 2/100\n",
      "4983/4983 [==============================] - 3s 535us/step - loss: 6.9235 - acc: 0.0427\n",
      "Epoch 3/100\n",
      "4983/4983 [==============================] - 3s 555us/step - loss: 6.8008 - acc: 0.0427\n",
      "Epoch 4/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 6.7214 - acc: 0.0427\n",
      "Epoch 5/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 6.6630 - acc: 0.0427\n",
      "Epoch 6/100\n",
      "4983/4983 [==============================] - 3s 568us/step - loss: 6.6165 - acc: 0.0427\n",
      "Epoch 7/100\n",
      "4983/4983 [==============================] - 3s 564us/step - loss: 6.5833 - acc: 0.0427\n",
      "Epoch 8/100\n",
      "4983/4983 [==============================] - 3s 566us/step - loss: 6.5449 - acc: 0.0427\n",
      "Epoch 9/100\n",
      "4983/4983 [==============================] - 3s 571us/step - loss: 6.5124 - acc: 0.0429\n",
      "Epoch 10/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 6.4826 - acc: 0.0427\n",
      "Epoch 11/100\n",
      "4983/4983 [==============================] - 3s 570us/step - loss: 6.4450 - acc: 0.0427\n",
      "Epoch 12/100\n",
      "4983/4983 [==============================] - 3s 595us/step - loss: 6.4121 - acc: 0.0429\n",
      "Epoch 13/100\n",
      "4983/4983 [==============================] - 4s 774us/step - loss: 6.3824 - acc: 0.0427\n",
      "Epoch 14/100\n",
      "4983/4983 [==============================] - 3s 701us/step - loss: 6.3526 - acc: 0.0427\n",
      "Epoch 15/100\n",
      "4983/4983 [==============================] - 3s 592us/step - loss: 6.3210 - acc: 0.0431\n",
      "Epoch 16/100\n",
      "4983/4983 [==============================] - 3s 610us/step - loss: 6.2911 - acc: 0.0431\n",
      "Epoch 17/100\n",
      "4983/4983 [==============================] - 3s 621us/step - loss: 6.2572 - acc: 0.0429\n",
      "Epoch 18/100\n",
      "4983/4983 [==============================] - 3s 590us/step - loss: 6.2218 - acc: 0.0431\n",
      "Epoch 19/100\n",
      "4983/4983 [==============================] - 3s 585us/step - loss: 6.1825 - acc: 0.0435\n",
      "Epoch 20/100\n",
      "4983/4983 [==============================] - 3s 588us/step - loss: 6.1425 - acc: 0.0435\n",
      "Epoch 21/100\n",
      "4983/4983 [==============================] - 3s 548us/step - loss: 6.0995 - acc: 0.0442\n",
      "Epoch 22/100\n",
      "4983/4983 [==============================] - 3s 563us/step - loss: 6.0532 - acc: 0.0433\n",
      "Epoch 23/100\n",
      "4983/4983 [==============================] - 3s 573us/step - loss: 6.0039 - acc: 0.0435\n",
      "Epoch 24/100\n",
      "4983/4983 [==============================] - 3s 551us/step - loss: 5.9495 - acc: 0.0448\n",
      "Epoch 25/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 5.8964 - acc: 0.0452\n",
      "Epoch 26/100\n",
      "4983/4983 [==============================] - 3s 571us/step - loss: 5.8370 - acc: 0.0458\n",
      "Epoch 27/100\n",
      "4983/4983 [==============================] - 3s 570us/step - loss: 5.7808 - acc: 0.0468\n",
      "Epoch 28/100\n",
      "4983/4983 [==============================] - 3s 573us/step - loss: 5.7191 - acc: 0.0468\n",
      "Epoch 29/100\n",
      "4983/4983 [==============================] - 3s 568us/step - loss: 5.6601 - acc: 0.0502\n",
      "Epoch 30/100\n",
      "4983/4983 [==============================] - 3s 570us/step - loss: 5.6041 - acc: 0.0492\n",
      "Epoch 31/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 5.5464 - acc: 0.0508\n",
      "Epoch 32/100\n",
      "4983/4983 [==============================] - 3s 552us/step - loss: 5.4913 - acc: 0.0526\n",
      "Epoch 33/100\n",
      "4983/4983 [==============================] - 3s 555us/step - loss: 5.4428 - acc: 0.0540\n",
      "Epoch 34/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 5.3852 - acc: 0.0590\n",
      "Epoch 35/100\n",
      "4983/4983 [==============================] - 3s 554us/step - loss: 5.3335 - acc: 0.0592\n",
      "Epoch 36/100\n",
      "4983/4983 [==============================] - 3s 559us/step - loss: 5.2823 - acc: 0.0620\n",
      "Epoch 37/100\n",
      "4983/4983 [==============================] - 3s 563us/step - loss: 5.2321 - acc: 0.0656\n",
      "Epoch 38/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 5.1781 - acc: 0.0694\n",
      "Epoch 39/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 5.1263 - acc: 0.0728\n",
      "Epoch 40/100\n",
      "4983/4983 [==============================] - 3s 558us/step - loss: 5.0779 - acc: 0.0734\n",
      "Epoch 41/100\n",
      "4983/4983 [==============================] - 3s 554us/step - loss: 5.0293 - acc: 0.0757\n",
      "Epoch 42/100\n",
      "4983/4983 [==============================] - 3s 551us/step - loss: 4.9802 - acc: 0.0751\n",
      "Epoch 43/100\n",
      "4983/4983 [==============================] - 3s 548us/step - loss: 4.9287 - acc: 0.0805\n",
      "Epoch 44/100\n",
      "4983/4983 [==============================] - 3s 552us/step - loss: 4.8807 - acc: 0.0841\n",
      "Epoch 45/100\n",
      "4983/4983 [==============================] - 3s 555us/step - loss: 4.8343 - acc: 0.0897\n",
      "Epoch 46/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 4.7845 - acc: 0.0907\n",
      "Epoch 47/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 4.7373 - acc: 0.0969\n",
      "Epoch 48/100\n",
      "4983/4983 [==============================] - 3s 554us/step - loss: 4.6935 - acc: 0.0991\n",
      "Epoch 49/100\n",
      "4983/4983 [==============================] - 3s 551us/step - loss: 4.6472 - acc: 0.1036\n",
      "Epoch 50/100\n",
      "4983/4983 [==============================] - 3s 554us/step - loss: 4.5966 - acc: 0.1086\n",
      "Epoch 51/100\n",
      "4983/4983 [==============================] - 3s 555us/step - loss: 4.5404 - acc: 0.1104\n",
      "Epoch 52/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 4.5013 - acc: 0.1168\n",
      "Epoch 53/100\n",
      "4983/4983 [==============================] - 3s 561us/step - loss: 4.4648 - acc: 0.1256\n",
      "Epoch 54/100\n",
      "4983/4983 [==============================] - 3s 557us/step - loss: 4.4109 - acc: 0.1322\n",
      "Epoch 55/100\n",
      "4983/4983 [==============================] - 3s 570us/step - loss: 4.3641 - acc: 0.1383\n",
      "Epoch 56/100\n",
      "4983/4983 [==============================] - 3s 573us/step - loss: 4.3316 - acc: 0.1381\n",
      "Epoch 57/100\n",
      "4983/4983 [==============================] - 3s 565us/step - loss: 4.2743 - acc: 0.1457\n",
      "Epoch 58/100\n",
      "4983/4983 [==============================] - 3s 564us/step - loss: 4.2389 - acc: 0.1521\n",
      "Epoch 59/100\n",
      "4983/4983 [==============================] - 3s 560us/step - loss: 4.1818 - acc: 0.1613\n",
      "Epoch 60/100\n",
      "4983/4983 [==============================] - 3s 564us/step - loss: 4.1362 - acc: 0.1642\n",
      "Epoch 61/100\n",
      "4983/4983 [==============================] - 3s 564us/step - loss: 4.0968 - acc: 0.1728\n",
      "Epoch 62/100\n",
      "4983/4983 [==============================] - 3s 565us/step - loss: 4.0566 - acc: 0.1826\n",
      "Epoch 63/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 4.0112 - acc: 0.1900\n",
      "Epoch 64/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 3.9694 - acc: 0.1953\n",
      "Epoch 65/100\n",
      "4983/4983 [==============================] - 3s 565us/step - loss: 3.9199 - acc: 0.2083\n",
      "Epoch 66/100\n",
      "4983/4983 [==============================] - 3s 575us/step - loss: 3.8757 - acc: 0.2212\n",
      "Epoch 67/100\n",
      "4983/4983 [==============================] - 3s 565us/step - loss: 3.8345 - acc: 0.2222\n",
      "Epoch 68/100\n",
      "4983/4983 [==============================] - 3s 566us/step - loss: 3.7951 - acc: 0.2250\n",
      "Epoch 69/100\n",
      "4983/4983 [==============================] - 3s 568us/step - loss: 3.7499 - acc: 0.2326\n",
      "Epoch 70/100\n",
      "4983/4983 [==============================] - 3s 569us/step - loss: 3.7132 - acc: 0.2422\n",
      "Epoch 71/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 3.6678 - acc: 0.2525\n",
      "Epoch 72/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 3.6126 - acc: 0.2619\n",
      "Epoch 73/100\n",
      "4983/4983 [==============================] - 3s 567us/step - loss: 3.5826 - acc: 0.2665\n",
      "Epoch 74/100\n",
      "4983/4983 [==============================] - 3s 570us/step - loss: 3.5442 - acc: 0.2729\n",
      "Epoch 75/100\n",
      "4983/4983 [==============================] - 3s 573us/step - loss: 3.4946 - acc: 0.2789\n",
      "Epoch 76/100\n",
      "4983/4983 [==============================] - 3s 574us/step - loss: 3.4614 - acc: 0.2912\n",
      "Epoch 77/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 3.4258 - acc: 0.2994\n",
      "Epoch 78/100\n",
      "4983/4983 [==============================] - 3s 571us/step - loss: 3.3873 - acc: 0.3060\n",
      "Epoch 79/100\n",
      "4983/4983 [==============================] - 3s 573us/step - loss: 3.3382 - acc: 0.3205\n",
      "Epoch 80/100\n",
      "4983/4983 [==============================] - 3s 575us/step - loss: 3.2977 - acc: 0.3183\n",
      "Epoch 81/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 3.2584 - acc: 0.3289\n",
      "Epoch 82/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 3.2229 - acc: 0.3379\n",
      "Epoch 83/100\n",
      "4983/4983 [==============================] - 3s 581us/step - loss: 3.1792 - acc: 0.3530\n",
      "Epoch 84/100\n",
      "4983/4983 [==============================] - 3s 583us/step - loss: 3.1416 - acc: 0.3624\n",
      "Epoch 85/100\n",
      "4983/4983 [==============================] - 3s 576us/step - loss: 3.1226 - acc: 0.3654\n",
      "Epoch 86/100\n",
      "4983/4983 [==============================] - 3s 579us/step - loss: 3.0729 - acc: 0.3713\n",
      "Epoch 87/100\n",
      "4983/4983 [==============================] - 3s 579us/step - loss: 3.0436 - acc: 0.3849\n",
      "Epoch 88/100\n",
      "4983/4983 [==============================] - 3s 595us/step - loss: 3.0068 - acc: 0.3895\n",
      "Epoch 89/100\n",
      "4983/4983 [==============================] - 3s 591us/step - loss: 2.9646 - acc: 0.4042\n",
      "Epoch 90/100\n",
      "4983/4983 [==============================] - 3s 579us/step - loss: 2.9248 - acc: 0.4132\n",
      "Epoch 91/100\n",
      "4983/4983 [==============================] - 3s 578us/step - loss: 2.8942 - acc: 0.4216\n",
      "Epoch 92/100\n",
      "4983/4983 [==============================] - 3s 582us/step - loss: 2.8591 - acc: 0.4198\n",
      "Epoch 93/100\n",
      "4983/4983 [==============================] - 3s 579us/step - loss: 2.8214 - acc: 0.4319\n",
      "Epoch 94/100\n",
      "4983/4983 [==============================] - 3s 584us/step - loss: 2.8059 - acc: 0.4351\n",
      "Epoch 95/100\n",
      "4983/4983 [==============================] - 3s 585us/step - loss: 2.7636 - acc: 0.4473\n",
      "Epoch 96/100\n",
      "4983/4983 [==============================] - 3s 585us/step - loss: 2.7286 - acc: 0.4483\n",
      "Epoch 97/100\n",
      "4983/4983 [==============================] - 3s 589us/step - loss: 2.6959 - acc: 0.4698\n",
      "Epoch 98/100\n",
      "4983/4983 [==============================] - 3s 585us/step - loss: 2.6584 - acc: 0.4646\n",
      "Epoch 99/100\n",
      "4983/4983 [==============================] - 3s 586us/step - loss: 2.6354 - acc: 0.4724\n",
      "Epoch 100/100\n",
      "4983/4983 [==============================] - 3s 582us/step - loss: 2.5898 - acc: 0.4895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168cea20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pd.DataFrame(X.values.tolist()), y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в будто же он уже сделал чтобы этого было объявили прежде очень него понравилось сказать не только чтобы и впредь только что уже мимо законного занятьем рубашками него обзавестись лице прибавить сыпью время платон и упомянутом мелели голубое и что такое благородство мыслей и никогда в много роде такого этого но было "
     ]
    }
   ],
   "source": [
    "#предсказываем для входа из 2 слов:\n",
    "last2 = data.iloc[np.random.randint(data.shape[0])]['target'][0]\n",
    "last1 = data.iloc[np.random.randint(data.shape[0])]['target'][0]\n",
    "print(last2, last1, end = ' ')\n",
    "MAXITER = 50\n",
    "reverse_dict = {v: k for k, v in tokenizer.word_index.items()}\n",
    "for i in range(MAXITER):\n",
    "    w2 = np.array([last2, last1])\n",
    "    tw2 = pd.DataFrame(tokenizer.texts_to_sequences(w2))\n",
    "    res_word = reverse_dict[model.predict_classes(pd.DataFrame(tw2.values.tolist()).T)[0]]\n",
    "    last2 = last1\n",
    "    last1 = res_word\n",
    "    print(last1, end = ' ')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
