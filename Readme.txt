Для выполнения задания использовался Jupiter Notebook. Для запуска необходимо:
1) запустить файл train_test_split.ipynb (Kernel -> Restart & Run All, либо просто сверху вниз по 1 ячейке).
2) (опционально, не является решением) Запустить файл Baseline.ipynb, в нём - быстро написанный baseline.
3) запустить файл LSTM+generation.ipynb, в этом файле находятся решения пунктов 2.1, 2.2, 2.3 задания.

Использованный подход:

Предобработка данных: были скачаны произведения Гоголя (Мёртвые души, Вий, Нос, Портрет, Вечера на хуторе близ Диканьки) и Гегеля (Феноменология духа, Наука логики, Философия права, Лекции по истории философии). Вручную были удалены (если присутствовали): рецензии, комментарии переводчиков, отзывы. В файле train_test_split описан препроцессинг. Текст при помощи nltk был разбит на предложения, был создан файл с данными формата (предложение - автор - номер произведения).

Разбитие данных на train и test:
У обоих авторов после обработки получилось примерно одинаковое количество предложений. Были выбраны схожие по объёму книги 2 авторов (Мёртвые души, Наука логики) и отправлены в test. Оставшиеся произведения были отправлены в train. На мой взгляд, такое разбиение является лучшим, чем разбиение на train и test каждой книги, при котором мы используем недоступную информацию (например, присутствие слова "Чичиков" однозначно укажет на авторство Гоголя).
 
Baseline решение:
В качестве ориентира была использована логистическая регрессия (без тюнинга и т. п.), натренированная на векторах предложений, полученных усреднением векторов слов (вытащенных из araneum - скачанной предварительно натренированной модели). На данной задаче она показала результат в 0.93 accuracy (93%).

Решение:
2.1:
Предложения были токенизированы на слова. Каждое слово было преобразовано в вектор при помощи araneum-модели (пробовал тренировать на своих текстах - качество сильно ухудшалось). Был использован pre-padding (post-padding давал похожий результат) с длиной последовательности в 25 токенов (слов; 25 в силу ограничений по памяти). При этом лемматизации и удаления стоп-слов не производилось (araneum учитывает форму в векторах слов; при удалений стоп-слов качество итоговой модели падало на 1% + увеличивалась "дисперсия"). Итоговой архитектурой стала схема LSTM-->Logit (1-слойная): Dropout и Recurrent-dropout уменьшали "дисперсию" модели, но вместе с тем и её качество (около 0.5%). Возможно, стоило попробовать увеличить кол-во слоёв, но при запуске LSTM (после подбора параметров) давала точность 96% на train и 95.5% на test, поэтому решил "оставить как есть".

2.2:
Были выбраны предложения, вероятность для которых была в интервале (0.5 - x, 0.5 + x), x = 0.01 для демонстрации; 0.1 - для задания 2.3.

2.3:
Из-за малого количества данных был выбран довольно простой метод: из выбранных в п. 2.2 предложений были созданы тройки (пред. слово, след. слово) - (текущее слово), на которых происходило обучение LSTM. При этом результат получился далёким от желаемого: при малом кол-ве эпох вывод нейросеть "зацикливалась" на 1 слове (поездки посвящен чиновников и и и и ...); при среднем - на 2 словах (также и также и также и ...); при большом количестве эпох начиналось переобучение (поездки посвящен чиновников что я не привез но гостинца он в нем а между тем заставляли подозревать что именно у ним и не видано было что это не принял в уваженье и тут же есть фабрики заметил платонов самоотверженья и высокой любви к добру и не видано было что это не принял). Возможно, стоило генерировать на всех текстах и "проверять" правильность, подавая результат на вход модели из 2.1, но из-за ограничений ПК идея осталась нереализованной.